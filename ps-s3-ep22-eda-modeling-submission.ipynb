{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/oscarm524/ps-s3-ep22-eda-modeling-submission?scriptVersionId=143106740\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<a id=\"table\"></a>\n<h1 style=\"background-color:lightgray;font-family:newtimeroman;font-size:350%;text-align:center;border-radius: 15px 50px;\">Table of Contents</h1>\n\n[1. Notebook Versions](#1)\n\n[2. Loading Libraries](#2)\n\n[3. Reading Data Files](#3)\n\n[4. Data Exploration](#4)\n\n[5. Baseline Modeling 1.0](#5)\n\n[6. Baseline Modeling 2.0](#6)\n\n\n<a id=\"1\"></a>\n# <h1 style=\"background-color:lightgray;font-family:newtimeroman;font-size:350%;text-align:center;border-radius: 15px 50px;\">Notebook Versions</h1>\n\n- Version 1 (09/11/2023)\n    * EDA \n    \n    \n- Version 2 (09/12/2023)\n    * EDA updated\n    \n    \n- Version 3 (09/12/2023)\n    * EDA updated\n    \n    \n- Version 4 (09/13/2023)\n    * Baseline modeling 1.0 added\n    \n    \n- Version 5 (09/15/2023)\n    * Baseline modeling 2.0 added\n    \n    \n- Version 6 (09/15/2023)\n    * Fixing bug\n    \n<a id=\"2\"></a>\n# <h1 style=\"background-color:lightgray;font-family:newtimeroman;font-size:350%;text-align:center;border-radius: 15px 50px;\">Loading Libraries</h1>    ","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd; pd.set_option('display.max_columns', 100)\nimport numpy as np\n\nfrom tqdm.notebook import tqdm\n\nimport re\n\nfrom functools import partial\nimport scipy as sp\nfrom scipy.stats import mode\n\nimport matplotlib.pyplot as plt; plt.style.use('ggplot')\nimport seaborn as sns\nimport plotly.express as px\n\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split, GridSearchCV, RepeatedStratifiedKFold\nfrom sklearn.metrics import roc_auc_score, roc_curve, RocCurveDisplay, cohen_kappa_score, log_loss, f1_score\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.feature_selection import RFE, RFECV\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.calibration import CalibrationDisplay\nfrom sklearn.inspection import PartialDependenceDisplay\nfrom sklearn.linear_model import LogisticRegression\nfrom collections import Counter\nfrom sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.svm import SVC\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier","metadata":{"execution":{"iopub.status.busy":"2023-09-15T12:26:39.704808Z","iopub.execute_input":"2023-09-15T12:26:39.705235Z","iopub.status.idle":"2023-09-15T12:26:39.720262Z","shell.execute_reply.started":"2023-09-15T12:26:39.7052Z","shell.execute_reply":"2023-09-15T12:26:39.719094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n# <h1 style=\"background-color:lightgray;font-family:newtimeroman;font-size:350%;text-align:center;border-radius: 15px 50px;\">Reading Data Files</h1> ","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/playground-series-s3e22/train.csv')\ntest = pd.read_csv('../input/playground-series-s3e22/test.csv')\nsubmission = pd.read_csv('../input/playground-series-s3e22/sample_submission.csv')\noriginal = pd.read_csv('../input/horse-survival-dataset/horse.csv')\n\nprint('The dimension of the train dataset is:', train.shape)\nprint('The dimension of the test dataset is:', test.shape)\nprint('The dimension of the original train dataset is:', original.shape)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T12:26:42.083772Z","iopub.execute_input":"2023-09-15T12:26:42.084243Z","iopub.status.idle":"2023-09-15T12:26:42.131631Z","shell.execute_reply.started":"2023-09-15T12:26:42.084202Z","shell.execute_reply":"2023-09-15T12:26:42.13033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n# <h1 style=\"background-color:lightgray;font-family:newtimeroman;font-size:350%;text-align:center;border-radius: 15px 50px;\">Data Exploration</h1>\n\nFirst, we explore the competition dataset. We start by visualizing `outcome`, the variable of interest.","metadata":{}},{"cell_type":"code","source":"sns.countplot(data = train, x = 'outcome')\nplt.ylabel('Frequency');","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-12T22:19:08.787352Z","iopub.execute_input":"2023-09-12T22:19:08.7878Z","iopub.status.idle":"2023-09-12T22:19:09.083725Z","shell.execute_reply.started":"2023-09-12T22:19:08.787765Z","shell.execute_reply":"2023-09-12T22:19:09.082326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above, the most frequent label is `lived`; on the other hand, `euthanized` is the least frequent label. Next, we explore relationships between the categorical features and `outcome`.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 2, figsize = (25, 17))\n\ncmap = sns.diverging_palette(100, 7, s = 75, l = 40, n = 5, center = 'light', as_cmap = True)\n\nsns.heatmap(data = pd.crosstab(train['surgery'], train['outcome']), annot = True, cmap = cmap, fmt = '.0f', ax = axes[0, 0])\nsns.heatmap(data = pd.crosstab(train['age'], train['outcome']), annot = True, cmap = cmap, fmt = '.0f', ax = axes[0, 1])\nsns.heatmap(data = pd.crosstab(train['temp_of_extremities'], train['outcome']), annot = True, cmap = cmap, fmt = '.0f', ax = axes[1, 0])\nsns.heatmap(data = pd.crosstab(train['peripheral_pulse'], train['outcome']), annot = True, cmap = cmap, fmt = '.0f', ax = axes[1, 1]);","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-12T22:19:11.015095Z","iopub.execute_input":"2023-09-12T22:19:11.015507Z","iopub.status.idle":"2023-09-12T22:19:12.906343Z","shell.execute_reply.started":"2023-09-12T22:19:11.015471Z","shell.execute_reply":"2023-09-12T22:19:12.905209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above heatmaps, these are some observations:\n\n- `young` horses are more likely to die.\n- horses with `temp_of_extremities = normal` are more likely to live.\n- horses with `peripheral_pulse = normal` are more likely to live.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 2, figsize = (25, 17))\n\nsns.heatmap(data = pd.crosstab(train['mucous_membrane'], train['outcome']), annot = True, cmap = cmap, fmt = '.0f', ax = axes[0, 0])\nsns.heatmap(data = pd.crosstab(train['capillary_refill_time'], train['outcome']), annot = True, cmap = cmap, fmt = '.0f', ax = axes[0, 1])\nsns.heatmap(data = pd.crosstab(train['pain'], train['outcome']), annot = True, cmap = cmap, fmt = '.0f', ax = axes[1, 0])\nsns.heatmap(data = pd.crosstab(train['peristalsis'], train['outcome']), annot = True, cmap = cmap, fmt = '.0f', ax = axes[1, 1]);","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-12T22:19:17.142611Z","iopub.execute_input":"2023-09-12T22:19:17.143001Z","iopub.status.idle":"2023-09-12T22:19:19.156001Z","shell.execute_reply.started":"2023-09-12T22:19:17.142969Z","shell.execute_reply":"2023-09-12T22:19:19.154871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above heatmaps, these are some observations:\n\n- horses with `mucous_membrane = normal_pink` are more likely to live.\n- Only two observations with `capillary_refill_time = 3`.\n- horses with `pain = mild_pain` are more likely to live.\n- Only one observation with `pain = slight`.\n- Only one observation with `peristalsis = distend_small`.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 2, figsize = (25, 17))\n\nsns.heatmap(data = pd.crosstab(train['abdominal_distention'], train['outcome']), annot = True, cmap = cmap, fmt = '.0f', ax = axes[0, 0])\nsns.heatmap(data = pd.crosstab(train['nasogastric_tube'], train['outcome']), annot = True, cmap = cmap, fmt = '.0f', ax = axes[0, 1])\nsns.heatmap(data = pd.crosstab(train['nasogastric_reflux'], train['outcome']), annot = True, cmap = cmap, fmt = '.0f', ax = axes[1, 0])\nsns.heatmap(data = pd.crosstab(train['rectal_exam_feces'], train['outcome']), annot = True, cmap = cmap, fmt = '.0f', ax = axes[1, 1]);","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-12T22:19:22.251742Z","iopub.execute_input":"2023-09-12T22:19:22.252166Z","iopub.status.idle":"2023-09-12T22:19:24.091066Z","shell.execute_reply.started":"2023-09-12T22:19:22.252131Z","shell.execute_reply":"2023-09-12T22:19:24.089998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above heatmaps, these are some observations:\n\n- horses with `abdominal_distention = slight` are more likely to live.\n- Only one observation with `nasogastric_reflux = slight`.\n- Only one observation with `rectal_exam_feces = serosanguious`.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 2, figsize = (25, 17))\n\nsns.heatmap(data = pd.crosstab(train['abdomen'], train['outcome']), annot = True, cmap = cmap, fmt = '.0f', ax = axes[0, 0])\nsns.heatmap(data = pd.crosstab(train['abdomo_appearance'], train['outcome']), annot = True, cmap = cmap, fmt = '.0f', ax = axes[0, 1])\nsns.heatmap(data = pd.crosstab(train['surgical_lesion'], train['outcome']), annot = True, cmap = cmap, fmt = '.0f', ax = axes[1, 0])\nsns.heatmap(data = pd.crosstab(train['cp_data'], train['outcome']), annot = True, cmap = cmap, fmt = '.0f', ax = axes[1, 1]);","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-12T22:19:27.875389Z","iopub.execute_input":"2023-09-12T22:19:27.875789Z","iopub.status.idle":"2023-09-12T22:19:29.777272Z","shell.execute_reply.started":"2023-09-12T22:19:27.87576Z","shell.execute_reply":"2023-09-12T22:19:29.776144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above heatmaps, these are some observations:\n\n- horses with `abdomo_appearance = clear` are more likely to live.\n- horses with `surgical_lesion = no` are more likely to live.\n\nNext, we explore potential relationships between the numeric input features and `outcome`.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 2, figsize = (25, 17))\n\nsns.boxplot(ax = axes[0, 0], data = train, x = 'outcome', y = 'rectal_temp');\nsns.boxplot(ax = axes[0, 1], data = train, x = 'outcome', y = 'pulse');\nsns.boxplot(ax = axes[1, 0], data = train, x = 'outcome', y = 'respiratory_rate');\nsns.boxplot(ax = axes[1, 1], data = train, x = 'outcome', y = 'nasogastric_reflux_ph');","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-12T22:19:33.521932Z","iopub.execute_input":"2023-09-12T22:19:33.523313Z","iopub.status.idle":"2023-09-12T22:19:34.448652Z","shell.execute_reply.started":"2023-09-12T22:19:33.523257Z","shell.execute_reply":"2023-09-12T22:19:34.447475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above boxplots, these are some observations:\n\n- `rectal_temp` distributions are similar across the three different labels of `outcome`.\n- The `pulse` of horses that lived, on average, is lower.\n- There is a slight downward trend in `respiratory_rate`, on average, from horses that died to horses that lived.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 2, figsize = (25, 17))\n\nsns.boxplot(ax = axes[0, 0], data = train, x = 'outcome', y = 'packed_cell_volume');\nsns.boxplot(ax = axes[0, 1], data = train, x = 'outcome', y = 'total_protein');\nsns.boxplot(ax = axes[1, 0], data = train, x = 'outcome', y = 'abdomo_protein');\nsns.boxplot(ax = axes[1, 1], data = train, x = 'outcome', y = 'lesion_1');","metadata":{"execution":{"iopub.status.busy":"2023-09-12T22:19:38.760375Z","iopub.execute_input":"2023-09-12T22:19:38.760783Z","iopub.status.idle":"2023-09-12T22:19:39.667864Z","shell.execute_reply.started":"2023-09-12T22:19:38.760749Z","shell.execute_reply":"2023-09-12T22:19:39.666701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above boxplots, these are some observations:\n\n- On average, `euthanized` horses have a higher `picked_cell_volume`.\n- On average, `euthanized` horses have a higher `total_protein`.\n\nNext, we explore potential relationship among the input features via pair-plots.","metadata":{}},{"cell_type":"code","source":"sns.pairplot(data = train[['rectal_temp', 'respiratory_rate', 'nasogastric_reflux_ph', 'packed_cell_volume', 'total_protein', 'abdomo_protein', 'outcome']], hue = 'outcome', corner = True); ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-12T22:21:16.501553Z","iopub.execute_input":"2023-09-12T22:21:16.502035Z","iopub.status.idle":"2023-09-12T22:21:28.1021Z","shell.execute_reply.started":"2023-09-12T22:21:16.501998Z","shell.execute_reply":"2023-09-12T22:21:28.101008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above scatter-plots, these are some observations:\n\n- From the `total_protein` scatter-plots, it is clear that there are two groups of observations: `total_protein < 40` and `total_protein > 40`. Also notice that, when `total_protein > 45` most of the horses either `lived` or `euthanized`. This is could be a potential important feature to engineer.\n\n- From the `nasogastric_reflux_ph` scatter-plots, we observe when `nasogastric_reflux_ph > 2` there are more red dots; that is, it seems that when `nasogastric_reflux_ph > 2` the likelihood of dying increases. ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n# <h1 style=\"background-color:lightgray;font-family:newtimeroman;font-size:350%;text-align:center;border-radius: 15px 50px;\">Baseline Modeling 1.0</h1>\n\nIn this section, we start building some models (without much feature engineering nor HPO). First, we start by defining the input and targe variables as shown below.","metadata":{}},{"cell_type":"code","source":"X = pd.concat([pd.get_dummies(train[['surgery', 'age', 'temp_of_extremities', 'peripheral_pulse', 'abdominal_distention', 'abdomo_appearance', 'surgical_lesion']]), train[['pulse', 'respiratory_rate', 'total_protein', 'nasogastric_reflux_ph']]], axis = 1)\n\nY = train['outcome']\nY = Y.map({'died': 0, 'euthanized': 1, 'lived': 2})\n\ntest_cv = pd.concat([pd.get_dummies(test[['surgery', 'age', 'temp_of_extremities', 'peripheral_pulse', 'abdominal_distention', 'abdomo_appearance', 'surgical_lesion']]), test[['pulse', 'respiratory_rate', 'total_protein', 'nasogastric_reflux_ph']]], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T12:05:41.04912Z","iopub.execute_input":"2023-09-15T12:05:41.049529Z","iopub.status.idle":"2023-09-15T12:05:41.089075Z","shell.execute_reply.started":"2023-09-15T12:05:41.049496Z","shell.execute_reply":"2023-09-15T12:05:41.088071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we run a simple 10-fold cross-validation routine to get an idea about model performance.","metadata":{}},{"cell_type":"code","source":"ens = list()\n\nsk = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 1, random_state = 42)\nfor i, (train_idx, test_idx) in enumerate(sk.split(X, Y)):\n\n    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n    Y_train, Y_test = Y.iloc[train_idx], Y.iloc[test_idx]\n\n    print('----------------------------------------------------------')\n    \n    ###################\n    ## Random Forest ##\n    ###################\n    \n    RF_md = RandomForestClassifier(n_estimators = 500, \n                                   max_depth = 7,\n                                   min_samples_split = 15,\n                                   min_samples_leaf = 10).fit(X_train, Y_train)\n\n    RF_pred = RF_md.predict(X_test)\n    RF_pred_test = RF_md.predict(test_cv)\n    RF_score = f1_score(Y_test, RF_pred, average = 'micro')\n\n    print('Fold', i, '==> RF oof F1 score is ==>', RF_score)\n\n    #################\n    ## Extra Trees ##\n    #################\n\n    ET_md = ExtraTreesClassifier(n_estimators = 500, \n                                 max_depth = 7,\n                                 min_samples_split = 15,\n                                 min_samples_leaf = 10).fit(X_train, Y_train)\n\n    ET_pred = ET_md.predict(X_test)\n    ET_pred_test = ET_md.predict(test_cv)\n    ET_score = f1_score(Y_test, ET_pred, average = 'micro')\n\n    print('Fold', i, '==> ET oof F1 score is ==>', ET_score)\n\n    ######################\n    ## GradientBoosting ##\n    ######################\n\n    GB_md = GradientBoostingClassifier(n_estimators = 500, \n                                       learning_rate = 0.01,\n                                       max_depth = 7,\n                                       min_samples_split = 15,\n                                       min_samples_leaf = 10).fit(X_train, Y_train)\n\n    GB_pred = GB_md.predict(X_test)\n    GB_pred_test = GB_md.predict(test_cv)\n    GB_score = f1_score(Y_test, GB_pred, average = 'micro')\n\n    print('Fold', i, '==> GB oof F1 score is ==>', GB_score)\n\n    ##########################\n    ## HistGradientBoosting ##\n    ##########################\n\n    hist_md = HistGradientBoostingClassifier(l2_regularization = 0.01,\n                                             early_stopping = False,\n                                             learning_rate = 0.01,\n                                             max_iter = 500,\n                                             max_depth = 7,\n                                             max_bins = 255,\n                                             min_samples_leaf = 5,\n                                             max_leaf_nodes = 5).fit(X_train, Y_train)\n    \n    hist_pred = hist_md.predict(X_test)\n    hist_pred_test = hist_md.predict(test_cv)\n    hist_score = f1_score(Y_test, hist_pred, average = 'micro')\n\n    print('Fold', i, '==> Hist oof F1 score is ==>', hist_score)   \n\n    ##########\n    ## LGBM ##\n    ##########\n\n    LGBM_md = LGBMClassifier(objective = 'multiclass',\n                             n_estimators = 500,\n                             max_depth = 7,\n                             learning_rate = 0.01,\n                             num_leaves = 20,\n                             reg_alpha = 3,\n                             reg_lambda = 3,\n                             subsample = 0.7,\n                             colsample_bytree = 0.7).fit(X_train, Y_train)\n\n    lgb_pred = LGBM_md.predict(X_test)\n    lgb_pred_test = LGBM_md.predict(test_cv)\n    lgb_score = f1_score(Y_test, lgb_pred, average = 'micro')\n\n    print('Fold', i, '==> LGBM oof F1 score is ==>', lgb_score)  \n\n    #########\n    ## XGB ##\n    #########\n\n    XGB_md = XGBClassifier(objective = 'multi:softprob',\n                           tree_method = 'hist',\n                           colsample_bytree = 0.7, \n                           gamma = 2, \n                           learning_rate = 0.01, \n                           max_depth = 7, \n                           min_child_weight = 10, \n                           n_estimators = 500, \n                           subsample = 0.7).fit(X_train, Y_train)\n\n    xgb_pred = XGB_md.predict(X_test)\n    xgb_pred_test = XGB_md.predict(test_cv)\n    xgb_score = f1_score(Y_test, xgb_pred, average = 'micro')\n\n    print('Fold', i, '==> XGB oof F1 score is ==>', xgb_score)\n\n    ##############\n    ## CatBoost ##\n    ##############\n\n    Cat_md = CatBoostClassifier(loss_function = 'MultiClass',\n                                iterations = 500,\n                                learning_rate = 0.01,\n                                depth = 7,\n                                random_strength = 0.5,\n                                bagging_temperature = 0.7,\n                                border_count = 30,\n                                l2_leaf_reg = 5,\n                                verbose = False, \n                                task_type = 'CPU').fit(X_train, Y_train)\n\n    cat_pred = Cat_md.predict(X_test)\n    cat_pred_test = Cat_md.predict(test_cv)\n    cat_score = f1_score(Y_test, cat_pred, average = 'micro')\n\n    print('Fold', i, '==> CatBoost oof F1 score is ==>', cat_score)\n\n    ###################\n    ## Mode Ensemble ##\n    ###################\n\n    md_preds = pd.concat([pd.Series(RF_pred.flatten()), \n                          pd.Series(ET_pred.flatten()), \n                          pd.Series(GB_pred.flatten()), \n                          pd.Series(hist_pred.flatten()), \n                          pd.Series(lgb_pred.flatten()), \n                          pd.Series(xgb_pred.flatten()),\n                          pd.Series(cat_pred.flatten())], axis = 1)\n    \n    md_preds_test = pd.concat([pd.Series(RF_pred_test.flatten()), \n                               pd.Series(ET_pred_test.flatten()), \n                               pd.Series(GB_pred_test.flatten()), \n                               pd.Series(hist_pred_test.flatten()), \n                               pd.Series(lgb_pred_test.flatten()), \n                               pd.Series(xgb_pred_test.flatten()),\n                               pd.Series(cat_pred_test.flatten())], axis = 1)\n\n    mode_ens = mode(md_preds, axis = 1, keepdims = True)[0]\n    mode_score = f1_score(Y_test, mode_ens, average = 'micro')\n    \n    mode_ens_test = mode(md_preds_test, axis = 1, keepdims = True)[0]\n    ens.append(mode_ens_test)\n    \n    print('Fold', i, '==> Mode Ensemble oof F1 score is ==>', mode_score)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T12:05:43.432123Z","iopub.execute_input":"2023-09-15T12:05:43.43255Z","iopub.status.idle":"2023-09-15T12:08:54.939357Z","shell.execute_reply.started":"2023-09-15T12:05:43.432516Z","shell.execute_reply":"2023-09-15T12:08:54.938032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we ensemble the predictions of the 10-folds via mode.","metadata":{}},{"cell_type":"code","source":"submission['outcome'] = mode(np.concatenate(ens, axis = 1), axis = 1, keepdims = True)[0]\nsubmission['outcome'] = submission['outcome'].map({0: 'died', 1: 'euthanized', 2: 'lived'})\nsubmission.to_csv('Baseline_Modeling_1.csv', index = False)\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T14:48:49.592268Z","iopub.execute_input":"2023-09-13T14:48:49.592685Z","iopub.status.idle":"2023-09-13T14:48:49.664553Z","shell.execute_reply.started":"2023-09-13T14:48:49.592652Z","shell.execute_reply":"2023-09-13T14:48:49.663531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6\"></a>\n# <h1 style=\"background-color:lightgray;font-family:newtimeroman;font-size:350%;text-align:center;border-radius: 15px 50px;\">Baseline Modeling 2.0</h1>\n\nIn this section, we use another set of input features with the goal of boosting the performance of the model. In this version, we consider less categorical featues as inputs (only `age` and `pain`). Notice we remove `pain_slight` from `train` and `pain_moderate` from `test`; those two labels only appear once in the `train` and `test` datasets, respectively. ","metadata":{}},{"cell_type":"code","source":"X = pd.concat([pd.get_dummies(train[['age', 'pain']]), train[['pulse', 'respiratory_rate', 'total_protein', 'abdomo_protein', 'nasogastric_reflux_ph']]], axis = 1)\nX = X.drop(columns = ['pain_slight'], axis = 1)\n\nY = train['outcome']\nY = Y.map({'died': 0, 'euthanized': 1, 'lived': 2})\n\ntest_cv = pd.concat([pd.get_dummies(test[['age', 'pain']]), test[['pulse', 'respiratory_rate', 'total_protein', 'abdomo_protein', 'nasogastric_reflux_ph']]], axis = 1)\ntest_cv = test_cv.drop(columns = ['pain_moderate'], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T12:09:59.269539Z","iopub.execute_input":"2023-09-15T12:09:59.270023Z","iopub.status.idle":"2023-09-15T12:09:59.292557Z","shell.execute_reply.started":"2023-09-15T12:09:59.269991Z","shell.execute_reply":"2023-09-15T12:09:59.291259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we run the usual cross-validation rountine. In this case, we include two more models: `LogisticRegression` and `LinearDiscriminatAnalysis`.","metadata":{}},{"cell_type":"code","source":"out = list()\nsk = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 1, random_state = 42)\nfor i, (train_idx, test_idx) in enumerate(sk.split(X, Y)):\n\n    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n    Y_train, Y_test = Y.iloc[train_idx], Y.iloc[test_idx]\n\n    print('----------------------------------------------------------')\n\n    ##############\n    ## Logistic ##\n    ##############\n\n    logit_md = make_pipeline(StandardScaler(), LogisticRegression(multi_class = 'multinomial')).fit(X_train, Y_train)\n\n    logit_pred = logit_md.predict(X_test)\n    logit_score = f1_score(Y_test, logit_pred, average = 'micro')\n\n    print('Fold', i, '==> Logistic oof F1 score is ==>', logit_score)\n\n    logit_pred_test = logit_md.predict(test_cv)\n\n    #########\n    ## LDA ##\n    #########\n\n    lda_md = make_pipeline(StandardScaler(), LinearDiscriminantAnalysis()).fit(X_train, Y_train)\n\n    lda_pred = lda_md.predict(X_test)\n    lda_score = f1_score(Y_test, lda_pred, average = 'micro')\n\n    print('Fold', i, '==> LDA oof F1 score is ==>', lda_score)\n\n    lda_pred_test = lda_md.predict(test_cv)\n    \n    ###################\n    ## Random Forest ##\n    ###################\n    \n    RF_md = RandomForestClassifier(n_estimators = 500, \n                                   max_depth = 5,\n                                   min_samples_split = 15,\n                                   min_samples_leaf = 10).fit(X_train, Y_train)\n\n    RF_pred = RF_md.predict(X_test)\n    RF_score = f1_score(Y_test, RF_pred, average = 'micro')\n\n    print('Fold', i, '==> RF oof F1 score is ==>', RF_score)\n\n    RF_pred_test = RF_md.predict(test_cv)\n\n    #################\n    ## Extra Trees ##\n    #################\n\n    ET_md = ExtraTreesClassifier(n_estimators = 500, \n                                 max_depth = 7,\n                                 min_samples_split = 15,\n                                 min_samples_leaf = 10).fit(X_train, Y_train)\n\n    ET_pred = ET_md.predict(X_test)\n    ET_score = f1_score(Y_test, ET_pred, average = 'micro')\n\n    print('Fold', i, '==> ET oof F1 score is ==>', ET_score)\n\n    ET_pred_test = ET_md.predict(test_cv)\n\n    ######################\n    ## GradientBoosting ##\n    ######################\n\n    GB_md = GradientBoostingClassifier(n_estimators = 500, \n                                       learning_rate = 0.01,\n                                       max_depth = 7,\n                                       min_samples_split = 15,\n                                       min_samples_leaf = 10).fit(X_train, Y_train)\n\n    GB_pred = GB_md.predict(X_test)\n    GB_score = f1_score(Y_test, GB_pred, average = 'micro')\n\n    print('Fold', i, '==> GB oof F1 score is ==>', GB_score)\n\n    GB_pred_test = GB_md.predict(test_cv)\n\n    ##########################\n    ## HistGradientBoosting ##\n    ##########################\n\n    hist_md = HistGradientBoostingClassifier(l2_regularization = 0.01,\n                                             early_stopping = False,\n                                             learning_rate = 0.01,\n                                             max_iter = 500,\n                                             max_depth = 7,\n                                             max_bins = 255,\n                                             min_samples_leaf = 5,\n                                             max_leaf_nodes = 5).fit(X_train, Y_train)\n    \n    hist_pred = hist_md.predict(X_test)\n    hist_score = f1_score(Y_test, hist_pred, average = 'micro')\n\n    print('Fold', i, '==> Hist oof F1 score is ==>', hist_score)  \n\n    hist_pred_test = hist_md.predict(test_cv)\n\n    ##########\n    ## LGBM ##\n    ##########\n\n    LGBM_md = LGBMClassifier(objective = 'multiclass',\n                             n_estimators = 500,\n                             max_depth = 7,\n                             learning_rate = 0.01,\n                             num_leaves = 20,\n                             reg_alpha = 3,\n                             reg_lambda = 3,\n                             subsample = 0.7,\n                             colsample_bytree = 0.7).fit(X_train, Y_train)\n\n    lgb_pred = LGBM_md.predict(X_test)\n    lgb_score = f1_score(Y_test, lgb_pred, average = 'micro')\n\n    print('Fold', i, '==> LGBM oof F1 score is ==>', lgb_score) \n\n    lgb_pred_test = LGBM_md.predict(test_cv)\n\n    #########\n    ## XGB ##\n    #########\n\n    XGB_md = XGBClassifier(objective = 'multi:softprob',\n                           tree_method = 'hist',\n                           colsample_bytree = 0.7, \n                           gamma = 2, \n                           learning_rate = 0.01, \n                           max_depth = 7, \n                           min_child_weight = 10, \n                           n_estimators = 500, \n                           subsample = 0.7).fit(X_train, Y_train)\n\n    xgb_pred = XGB_md.predict(X_test)\n    xgb_score = f1_score(Y_test, xgb_pred, average = 'micro')\n\n    print('Fold', i, '==> XGB oof F1 score is ==>', xgb_score)\n\n    xgb_pred_test = XGB_md.predict(test_cv)\n\n    ##############\n    ## CatBoost ##\n    ##############\n\n    Cat_md = CatBoostClassifier(loss_function = 'MultiClass',\n                                iterations = 500,\n                                learning_rate = 0.01,\n                                depth = 7,\n                                random_strength = 0.5,\n                                bagging_temperature = 0.7,\n                                border_count = 30,\n                                l2_leaf_reg = 5,\n                                verbose = False, \n                                task_type = 'CPU').fit(X_train, Y_train)\n\n    cat_pred = Cat_md.predict(X_test)\n    cat_score = f1_score(Y_test, cat_pred, average = 'micro')\n\n    print('Fold', i, '==> CatBoost oof F1 score is ==>', cat_score)\n\n    cat_pred_test = Cat_md.predict(test_cv)\n\n    ###################\n    ## Mode Ensemble ##\n    ###################\n\n    md_preds = pd.concat([pd.Series(logit_pred.flatten()),\n                          pd.Series(lda_pred.flatten()),\n                          pd.Series(RF_pred.flatten()), \n                          pd.Series(ET_pred.flatten()), \n                          pd.Series(GB_pred.flatten()), \n                          pd.Series(hist_pred.flatten()), \n                          pd.Series(lgb_pred.flatten()), \n                          pd.Series(xgb_pred.flatten()),\n                          pd.Series(cat_pred.flatten())], axis = 1)\n\n    md_preds_test = pd.concat([pd.Series(logit_pred_test.flatten()),\n                               pd.Series(lda_pred_test.flatten()), \n                               pd.Series(RF_pred_test.flatten()), \n                               pd.Series(ET_pred_test.flatten()), \n                               pd.Series(GB_pred_test.flatten()), \n                               pd.Series(hist_pred_test.flatten()), \n                               pd.Series(lgb_pred_test.flatten()), \n                               pd.Series(xgb_pred_test.flatten()),\n                               pd.Series(cat_pred_test.flatten())], axis = 1)\n\n\n    mode_ens = mode(md_preds, axis = 1, keepdims = True)[0]\n    mode_score = f1_score(Y_test, mode_ens, average = 'micro')\n\n    print('Fold', i, '==> Mode Ensemble oof F1 score is ==>', mode_score)\n\n    out.append(mode(md_preds_test, axis = 1, keepdims = True)[0])","metadata":{"execution":{"iopub.status.busy":"2023-09-15T12:11:17.468068Z","iopub.execute_input":"2023-09-15T12:11:17.46849Z","iopub.status.idle":"2023-09-15T12:13:57.847683Z","shell.execute_reply.started":"2023-09-15T12:11:17.468447Z","shell.execute_reply":"2023-09-15T12:13:57.846696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we ensemble the predictions of the 10-folds via mode.","metadata":{}},{"cell_type":"code","source":"submission['outcome'] = mode(np.concatenate(out, axis = 1), axis = 1, keepdims = True)[0]\nsubmission['outcome'] = submission['outcome'].map({0: 'died', 1: 'euthanized', 2: 'lived'})\nsubmission.to_csv('Baseline_Modeling_2.csv', index = False)\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T12:15:05.420968Z","iopub.execute_input":"2023-09-15T12:15:05.42137Z","iopub.status.idle":"2023-09-15T12:15:05.49447Z","shell.execute_reply.started":"2023-09-15T12:15:05.42134Z","shell.execute_reply":"2023-09-15T12:15:05.493033Z"},"trusted":true},"execution_count":null,"outputs":[]}]}