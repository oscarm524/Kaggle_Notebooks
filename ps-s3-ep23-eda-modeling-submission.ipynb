{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id=\"table\"></a>\n<h1 style=\"background-color:lightgray;font-family:newtimeroman;font-size:350%;text-align:center;border-radius: 15px 50px;\">Table of Contents</h1>\n\n[1. Notebook Versions](#1)\n\n[2. Loading Libraries](#2)\n\n[3. Reading Data Files](#3)\n\n[4. Data Exploration](#4)\n\n[5. Baseline Modeling 1.0](#5)\n\n\n<a id=\"1\"></a>\n# <h1 style=\"background-color:lightgray;font-family:newtimeroman;font-size:350%;text-align:center;border-radius: 15px 50px;\">Notebook Versions</h1>\n\n- Version 1 (10/02/2023)\n    * EDA \n    * Baseline modeling 1.0\n            \n        \n<a id=\"2\"></a>\n# <h1 style=\"background-color:lightgray;font-family:newtimeroman;font-size:350%;text-align:center;border-radius: 15px 50px;\">Loading Libraries</h1>    ","metadata":{}},{"cell_type":"code","source":"import pandas as pd; pd.set_option('display.max_columns', 100)\nimport numpy as np\n\nfrom tqdm.notebook import tqdm\n\nimport re\n\nfrom functools import partial\nfrom scipy.stats import mode\n\nimport matplotlib.pyplot as plt; plt.style.use('ggplot')\nimport seaborn as sns\nimport plotly.express as px\n\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import KNNImputer\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split, GridSearchCV, RepeatedStratifiedKFold\nfrom sklearn.metrics import roc_auc_score, roc_curve, RocCurveDisplay, cohen_kappa_score, log_loss, f1_score\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.feature_selection import RFE, RFECV\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.calibration import CalibrationDisplay\nfrom sklearn.inspection import PartialDependenceDisplay, permutation_importance\nfrom sklearn.linear_model import LogisticRegression\nfrom collections import Counter\nfrom sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.svm import SVC\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\nimport optuna","metadata":{"execution":{"iopub.status.busy":"2023-10-03T02:47:22.675399Z","iopub.execute_input":"2023-10-03T02:47:22.675828Z","iopub.status.idle":"2023-10-03T02:47:29.386249Z","shell.execute_reply.started":"2023-10-03T02:47:22.675795Z","shell.execute_reply":"2023-10-03T02:47:29.385472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n# <h1 style=\"background-color:lightgray;font-family:newtimeroman;font-size:350%;text-align:center;border-radius: 15px 50px;\">Reading Data Files</h1> ","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/playground-series-s3e23/train.csv')\ntest = pd.read_csv('../input/playground-series-s3e23/test.csv')\nsubmission = pd.read_csv('../input/playground-series-s3e23/sample_submission.csv')\n\nprint('The dimension of the train dataset is:', train.shape)\nprint('The dimension of the test dataset is:', test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T03:10:55.235298Z","iopub.execute_input":"2023-10-03T03:10:55.235614Z","iopub.status.idle":"2023-10-03T03:10:55.601694Z","shell.execute_reply.started":"2023-10-03T03:10:55.235590Z","shell.execute_reply":"2023-10-03T03:10:55.600625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2023-10-03T00:49:06.545893Z","iopub.execute_input":"2023-10-03T00:49:06.546313Z","iopub.status.idle":"2023-10-03T00:49:06.714717Z","shell.execute_reply.started":"2023-10-03T00:49:06.546283Z","shell.execute_reply":"2023-10-03T00:49:06.713402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2023-10-03T00:49:21.989673Z","iopub.execute_input":"2023-10-03T00:49:21.990035Z","iopub.status.idle":"2023-10-03T00:49:22.023638Z","shell.execute_reply.started":"2023-10-03T00:49:21.990007Z","shell.execute_reply":"2023-10-03T00:49:22.022908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are not missing values in the `train` dataset. ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n# <h1 style=\"background-color:lightgray;font-family:newtimeroman;font-size:350%;text-align:center;border-radius: 15px 50px;\">Data Exploration</h1>\n\nFirst, we start by visualizing `defects`, the variable of interest.","metadata":{}},{"cell_type":"code","source":"train['defects'].value_counts(normalize = True).plot(kind = 'bar', color = ['steelblue', 'orange'])\nplt.ylabel('Percentage');","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:04:24.073175Z","iopub.execute_input":"2023-10-03T01:04:24.073934Z","iopub.status.idle":"2023-10-03T01:04:24.313875Z","shell.execute_reply.started":"2023-10-03T01:04:24.073896Z","shell.execute_reply":"2023-10-03T01:04:24.312141Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above barplot, we see the data is imbalanced (~77% False and ~23% True). Next, we proceed to explore potential correlations among the input features.","metadata":{}},{"cell_type":"code","source":"corr_mat = train.drop(columns = ['id', 'defects'], axis = 1).corr()\n\ndata_mask = np.triu(np.ones_like(corr_mat, dtype = bool))\ncmap = sns.diverging_palette(100, 7, s = 75, l = 40, n = 20, center = 'light', as_cmap = True)\nf, ax = plt.subplots(figsize = (18, 13))\nsns.heatmap(corr_mat, annot = True, cmap = cmap, fmt = '.2f', center = 0,\n            annot_kws = {'size': 12}, mask = data_mask).set_title('Correlations Among Input Features');\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:20:00.537152Z","iopub.execute_input":"2023-10-03T01:20:00.537498Z","iopub.status.idle":"2023-10-03T01:20:02.668766Z","shell.execute_reply.started":"2023-10-03T01:20:00.537474Z","shell.execute_reply":"2023-10-03T01:20:02.667624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above, these are couple of observations:\n\n- There is a 97% correlation between `branchCount` and `v(g)`.\n- There is a 96% correlation between `total_Opnd` and `total_Op`\n- There is a 96% correlation between `total_Op` and `n`\n- `l` is the only feature that is negative correlated with the other features.\n\nBased on the above correlation heatmap, we proceed to explore the idea of dimension reduction via `PCA`.","metadata":{}},{"cell_type":"code","source":"pca_md = Pipeline([('scaler', StandardScaler()), \n                   ('PCA', PCA())]).fit(train.drop(columns = ['id', 'defects'], axis = 1))\npca_md","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:48:52.478145Z","iopub.execute_input":"2023-10-03T01:48:52.478514Z","iopub.status.idle":"2023-10-03T01:48:52.650583Z","shell.execute_reply.started":"2023-10-03T01:48:52.478488Z","shell.execute_reply":"2023-10-03T01:48:52.649215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (10, 8))\nax = sns.lineplot(x = [i for i in range(1, 22)], y = np.cumsum(pca_md.named_steps['PCA'].explained_variance_ratio_), color = 'steelblue', markers = True);\nax.set_xlabel('Number of Components')\nax.set_ylabel('Explained Variance (%)')\nax.set_xticks([i for i in range(1, 22)]);","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:56:28.817215Z","iopub.execute_input":"2023-10-03T01:56:28.817654Z","iopub.status.idle":"2023-10-03T01:56:29.173087Z","shell.execute_reply.started":"2023-10-03T01:56:28.817622Z","shell.execute_reply":"2023-10-03T01:56:29.171918Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above, 15 components explain about 98.5% of the variability in the data. ","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize = (20,8))\n\nsns.scatterplot(ax = axes[0], data = train, x = 'uniq_Op', y = 'uniq_Opnd', hue = 'defects');\nsns.scatterplot(ax = axes[1], data = train, x = 'total_Op', y = 'total_Opnd', hue = 'defects');","metadata":{"execution":{"iopub.status.busy":"2023-10-03T02:13:42.763715Z","iopub.execute_input":"2023-10-03T02:13:42.764200Z","iopub.status.idle":"2023-10-03T02:13:53.207137Z","shell.execute_reply.started":"2023-10-03T02:13:42.764158Z","shell.execute_reply":"2023-10-03T02:13:53.206140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above plots, these are a couple of observations:\n\n- There are several outliers in this data. \n- There is not a clear pattern that can be leveraged to separate the two classes.\n\nNext, we count the number of unique values in each of the input features as follows.","metadata":{}},{"cell_type":"code","source":"train.drop(columns = ['id', 'defects'], axis = 1).nunique()","metadata":{"execution":{"iopub.status.busy":"2023-10-03T02:22:22.265969Z","iopub.execute_input":"2023-10-03T02:22:22.266424Z","iopub.status.idle":"2023-10-03T02:22:22.310108Z","shell.execute_reply.started":"2023-10-03T02:22:22.266392Z","shell.execute_reply":"2023-10-03T02:22:22.308725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above, notice that `locCodeAndComment` is the feature with the least number of unique values; it has only 29 unique values. ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n# <h1 style=\"background-color:lightgray;font-family:newtimeroman;font-size:350%;text-align:center;border-radius: 15px 50px;\">Baseline Modeling 1.0</h1>\n\nFirst, we start by building some standard models without feature engineering nor HPO. First, we define the input and target features.","metadata":{}},{"cell_type":"code","source":"X = train.drop(columns = ['id', 'defects'], axis = 1)\nY = train['defects'].map({False: 0, True: 1})\n\ntest_cv = test.drop(columns = ['id'], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T02:53:51.429062Z","iopub.execute_input":"2023-10-03T02:53:51.429398Z","iopub.status.idle":"2023-10-03T02:53:51.447389Z","shell.execute_reply.started":"2023-10-03T02:53:51.429369Z","shell.execute_reply":"2023-10-03T02:53:51.446249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we build a few standard models in a 10-fold cross-validation routine.","metadata":{}},{"cell_type":"code","source":"ens_cv_scores, ens_preds = list(), list()\n\nsk = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 1, random_state = 42)\nfor i, (train_idx, test_idx) in enumerate(sk.split(X, Y)):\n\n    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n    Y_train, Y_test = Y.iloc[train_idx], Y.iloc[test_idx]\n    \n    print('----------------------------------------------------------')\n    \n    ########\n    ## RF ##\n    ########\n\n    RF_md = RandomForestClassifier(n_estimators = 500, \n                                   max_depth = 7,\n                                   min_samples_split = 15,\n                                   min_samples_leaf = 10).fit(X_train, Y_train)\n    \n    RF_pred = RF_md.predict_proba(X_test)[:, 1]\n    RF_score = roc_auc_score(Y_test, RF_pred)\n\n    print('Fold', i, '==> RF oof ROC-AUC score is ==>', RF_score)\n\n    RF_pred_test = RF_md.predict_proba(test_cv)[:, 1]\n    \n    #################\n    ## Extra Trees ##\n    #################\n\n    ET_md = ExtraTreesClassifier(n_estimators = 500, \n                                 max_depth = 7,\n                                 min_samples_split = 15,\n                                 min_samples_leaf = 10).fit(X_train, Y_train)\n\n    ET_pred = ET_md.predict_proba(X_test)[:, 1]\n    ET_score = roc_auc_score(Y_test, ET_pred)\n\n    print('Fold', i, '==> ET oof ROC-AUC score is ==>', ET_score)\n\n    ET_pred_test = ET_md.predict_proba(test_cv)[:, 1]\n\n    ##########################\n    ## HistGradientBoosting ##\n    ##########################\n\n    hist_md = HistGradientBoostingClassifier(l2_regularization = 0.01,\n                                             early_stopping = False,\n                                             learning_rate = 0.01,\n                                             max_iter = 500,\n                                             max_depth = 5,\n                                             max_bins = 255,\n                                             min_samples_leaf = 15,\n                                             max_leaf_nodes = 10).fit(X_train, Y_train)\n    \n    hist_pred = hist_md.predict_proba(X_test)[:, 1]\n    hist_score = roc_auc_score(Y_test, hist_pred)\n\n    print('Fold', i, '==> Hist oof ROC-AUC score is ==>', hist_score)  \n\n    hist_pred_test = hist_md.predict_proba(test_cv)[:, 1]\n\n    ##########\n    ## LGBM ##\n    ##########\n\n    LGBM_md = LGBMClassifier(objective = 'binary',\n                             n_estimators = 500,\n                             max_depth = 7,\n                             learning_rate = 0.01,\n                             num_leaves = 20,\n                             reg_alpha = 3,\n                             reg_lambda = 3,\n                             subsample = 0.7,\n                             colsample_bytree = 0.7).fit(X_train, Y_train)\n\n    lgb_pred = LGBM_md.predict_proba(X_test)[:, 1]\n    lgb_score = roc_auc_score(Y_test, lgb_pred)\n\n    print('Fold', i, '==> LGBM oof ROC-AUC score is ==>', lgb_score) \n\n    lgb_pred_test = LGBM_md.predict_proba(test_cv)[:, 1]\n\n    #########\n    ## XGB ##\n    #########\n\n    XGB_md = XGBClassifier(objective = 'binary:logistic',\n                           tree_method = 'hist',\n                           colsample_bytree = 0.7, \n                           gamma = 2, \n                           learning_rate = 0.01, \n                           max_depth = 7, \n                           min_child_weight = 10, \n                           n_estimators = 500, \n                           subsample = 0.7).fit(X_train, Y_train)\n\n    xgb_pred = XGB_md.predict_proba(X_test)[:, 1]\n    xgb_score = roc_auc_score(Y_test, xgb_pred)\n\n    print('Fold', i, '==> XGB oof ROC-AUC score is ==>', xgb_score)\n\n    xgb_pred_test = XGB_md.predict_proba(test_cv)[:, 1]\n\n    ##############\n    ## CatBoost ##\n    ##############\n\n    Cat_md = CatBoostClassifier(loss_function = 'Logloss',\n                                iterations = 500,\n                                learning_rate = 0.01,\n                                depth = 7,\n                                random_strength = 0.5,\n                                bagging_temperature = 0.7,\n                                border_count = 30,\n                                l2_leaf_reg = 5,\n                                verbose = False, \n                                task_type = 'CPU').fit(X_train, Y_train)\n\n    cat_pred = Cat_md.predict_proba(X_test)[:, 1]\n    cat_score = roc_auc_score(Y_test, cat_pred)\n\n    print('Fold', i, '==> CatBoost oof ROC-AUC score is ==>', cat_score)\n\n    cat_pred_test = Cat_md.predict_proba(test_cv)[:, 1]    \n    \n    ##############\n    ## Ensemble ##\n    ##############\n    \n    ens_pred_1 = (RF_pred + ET_pred + hist_pred + lgb_pred + xgb_pred + cat_pred) / 6\n    ens_pred_2 = (RF_pred_test + ET_pred_test + hist_pred_test + lgb_pred_test + xgb_pred_test + cat_pred_test) / 6\n    \n    ens_score_fold = roc_auc_score(Y_test, ens_pred_1)\n    ens_cv_scores.append(ens_score_fold)\n    ens_preds.append(ens_pred_2)\n    \n    print('Fold', i, '==> Average Ensemble oof ROC-AUC score is ==>', ens_score_fold)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T02:55:54.507361Z","iopub.execute_input":"2023-10-03T02:55:54.507939Z","iopub.status.idle":"2023-10-03T03:10:10.783412Z","shell.execute_reply.started":"2023-10-03T02:55:54.507909Z","shell.execute_reply":"2023-10-03T03:10:10.782289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('The average ROC-AUC score over the 10-folds is', np.mean(ens_cv_scores))","metadata":{"execution":{"iopub.status.busy":"2023-10-03T03:10:14.722641Z","iopub.execute_input":"2023-10-03T03:10:14.723395Z","iopub.status.idle":"2023-10-03T03:10:14.727704Z","shell.execute_reply.started":"2023-10-03T03:10:14.723359Z","shell.execute_reply":"2023-10-03T03:10:14.727036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ens_preds_test = pd.DataFrame(ens_preds).apply(np.mean, axis = 0)\n\nsubmission['defects'] = ens_preds_test\nsubmission.to_csv('Ensemble_Baseline_submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T03:11:01.836641Z","iopub.execute_input":"2023-10-03T03:11:01.837124Z","iopub.status.idle":"2023-10-03T03:11:08.502184Z","shell.execute_reply.started":"2023-10-03T03:11:01.837092Z","shell.execute_reply":"2023-10-03T03:11:08.501115Z"},"trusted":true},"execution_count":null,"outputs":[]}]}