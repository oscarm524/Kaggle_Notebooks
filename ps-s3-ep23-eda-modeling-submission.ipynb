{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/oscarm524/ps-s3-ep23-eda-modeling-submission?scriptVersionId=145238976\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<a id=\"table\"></a>\n<h1 style=\"background-color:lightgray;font-family:newtimeroman;font-size:350%;text-align:center;border-radius: 15px 50px;\">Table of Contents</h1>\n\n[1. Notebook Versions](#1)\n\n[2. Loading Libraries](#2)\n\n[3. Reading Data Files](#3)\n\n[4. Data Exploration](#4)\n\n[5. Baseline Modeling 1.0](#5)\n\n\n<a id=\"1\"></a>\n# <h1 style=\"background-color:lightgray;font-family:newtimeroman;font-size:350%;text-align:center;border-radius: 15px 50px;\">Notebook Versions</h1>\n\n- Version 1 (10/02/2023)\n    * EDA \n    * Baseline modeling 1.0\n            \n            \n- Version 2 (10/03/2023)\n    * Baseline modeling 1.0 updated (hill climbing added)\n    \n    \n- Version 3 (10/04/2023)\n    * EDA updated\n        \n<a id=\"2\"></a>\n# <h1 style=\"background-color:lightgray;font-family:newtimeroman;font-size:350%;text-align:center;border-radius: 15px 50px;\">Loading Libraries</h1>    ","metadata":{}},{"cell_type":"code","source":"import pandas as pd; pd.set_option('display.max_columns', 100)\nimport numpy as np\n\nfrom tqdm.notebook import tqdm\n\nimport re\n\nfrom functools import partial\nfrom scipy.stats import mode\n\nimport matplotlib.pyplot as plt; plt.style.use('ggplot')\nimport seaborn as sns\nimport plotly.express as px\n\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import KNNImputer\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split, GridSearchCV, RepeatedStratifiedKFold\nfrom sklearn.metrics import roc_auc_score, roc_curve, RocCurveDisplay, cohen_kappa_score, log_loss, f1_score\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.feature_selection import RFE, RFECV\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.calibration import CalibrationDisplay\nfrom sklearn.inspection import PartialDependenceDisplay, permutation_importance\nfrom sklearn.linear_model import LogisticRegression\nfrom collections import Counter\nfrom sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.svm import SVC\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\nimport optuna","metadata":{"execution":{"iopub.status.busy":"2023-10-04T15:17:12.480662Z","iopub.execute_input":"2023-10-04T15:17:12.481046Z","iopub.status.idle":"2023-10-04T15:17:19.119062Z","shell.execute_reply.started":"2023-10-04T15:17:12.481013Z","shell.execute_reply":"2023-10-04T15:17:19.118009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n# <h1 style=\"background-color:lightgray;font-family:newtimeroman;font-size:350%;text-align:center;border-radius: 15px 50px;\">Reading Data Files</h1> ","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/playground-series-s3e23/train.csv')\ntest = pd.read_csv('../input/playground-series-s3e23/test.csv')\nsubmission = pd.read_csv('../input/playground-series-s3e23/sample_submission.csv')\n\nprint('The dimension of the train dataset is:', train.shape)\nprint('The dimension of the test dataset is:', test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T15:17:27.295306Z","iopub.execute_input":"2023-10-04T15:17:27.295844Z","iopub.status.idle":"2023-10-04T15:17:28.018881Z","shell.execute_reply.started":"2023-10-04T15:17:27.295802Z","shell.execute_reply":"2023-10-04T15:17:28.017791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2023-10-04T15:17:30.777794Z","iopub.execute_input":"2023-10-04T15:17:30.778196Z","iopub.status.idle":"2023-10-04T15:17:30.961102Z","shell.execute_reply.started":"2023-10-04T15:17:30.778162Z","shell.execute_reply":"2023-10-04T15:17:30.959865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2023-10-04T15:17:33.740053Z","iopub.execute_input":"2023-10-04T15:17:33.740432Z","iopub.status.idle":"2023-10-04T15:17:33.769373Z","shell.execute_reply.started":"2023-10-04T15:17:33.740402Z","shell.execute_reply":"2023-10-04T15:17:33.768524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.describe()","metadata":{"execution":{"iopub.status.busy":"2023-10-04T15:18:03.11841Z","iopub.execute_input":"2023-10-04T15:18:03.118779Z","iopub.status.idle":"2023-10-04T15:18:03.229159Z","shell.execute_reply.started":"2023-10-04T15:18:03.118752Z","shell.execute_reply":"2023-10-04T15:18:03.228119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are not missing values neither in the `train` nor `test` datasets. Since this is synthetic dataset, as a sanity check, we will check for duplicates. First, let's take a look at the `train` dataset.","metadata":{}},{"cell_type":"code","source":"print(f\"If we remove id from the train dataset the dimension is {train.drop(columns = ['id'], axis = 1).drop_duplicates().shape}\")\nprint(f\"If we remove id and defects from the train dataset the dimension is {train.drop(columns = ['id', 'defects'], axis = 1).drop_duplicates().shape}\")\nprint(f\"If we remove id, branchCount, and defects from the train dataset the dimension is {train.drop(columns = ['id', 'branchCount', 'defects'], axis = 1).drop_duplicates().shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-04T15:33:55.260074Z","iopub.execute_input":"2023-10-04T15:33:55.260851Z","iopub.status.idle":"2023-10-04T15:33:55.479934Z","shell.execute_reply.started":"2023-10-04T15:33:55.260812Z","shell.execute_reply":"2023-10-04T15:33:55.479053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above, we see thera are 78 quasi-duplicated observations in the `train` dataset. Now, let's take a look at the `test` dataset.","metadata":{}},{"cell_type":"code","source":"print(f\"If we remove id from the test dataset the dimension is {test.drop(columns = ['id'], axis = 1).drop_duplicates().shape}\")\nprint(f\"If we remove id and defects from the test dataset the dimension is {test.drop(columns = ['id', 'branchCount'], axis = 1).drop_duplicates().shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-04T15:33:46.841377Z","iopub.execute_input":"2023-10-04T15:33:46.841782Z","iopub.status.idle":"2023-10-04T15:33:46.929061Z","shell.execute_reply.started":"2023-10-04T15:33:46.841752Z","shell.execute_reply":"2023-10-04T15:33:46.92789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above, we see that there are 38 quasi-duplicated observations in the `test` dataset. Finally, let's check if there are observations that appear in both the `train` and `test` datasets.","metadata":{}},{"cell_type":"code","source":"to_check = pd.merge(train.drop(columns = ['id', 'defects'], axis = 1), test.drop(columns = 'id', axis = 1))\nprint(f\"There are {to_check.shape[0]} duplicated observations in the train and test datasets\")\n\nto_check = pd.merge(train.drop(columns = ['id', 'branchCount', 'defects'], axis = 1), test.drop(columns = ['id', 'branchCount'], axis = 1), on = test.drop(columns = ['id', 'branchCount'], axis = 1).columns.tolist())\nprint(f\"There are {to_check.drop_duplicates().shape[0]} quasi-duplicated observations in the train and test datasets\")","metadata":{"execution":{"iopub.status.busy":"2023-10-04T15:37:22.186457Z","iopub.execute_input":"2023-10-04T15:37:22.186812Z","iopub.status.idle":"2023-10-04T15:37:22.474451Z","shell.execute_reply.started":"2023-10-04T15:37:22.186785Z","shell.execute_reply":"2023-10-04T15:37:22.47341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n# <h1 style=\"background-color:lightgray;font-family:newtimeroman;font-size:350%;text-align:center;border-radius: 15px 50px;\">Data Exploration</h1>\n\nFirst, we start by visualizing `defects`, the variable of interest.","metadata":{}},{"cell_type":"code","source":"train['defects'].value_counts(normalize = True).plot(kind = 'bar', color = ['steelblue', 'orange'])\nplt.ylabel('Percentage');","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:04:24.073175Z","iopub.execute_input":"2023-10-03T01:04:24.073934Z","iopub.status.idle":"2023-10-03T01:04:24.313875Z","shell.execute_reply.started":"2023-10-03T01:04:24.073896Z","shell.execute_reply":"2023-10-03T01:04:24.312141Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above barplot, we see the data is imbalanced (~77% False and ~23% True). Next, we proceed to explore potential correlations among the input features.","metadata":{}},{"cell_type":"code","source":"corr_mat = train.drop(columns = ['id', 'defects'], axis = 1).corr()\n\ndata_mask = np.triu(np.ones_like(corr_mat, dtype = bool))\ncmap = sns.diverging_palette(100, 7, s = 75, l = 40, n = 20, center = 'light', as_cmap = True)\nf, ax = plt.subplots(figsize = (18, 13))\nsns.heatmap(corr_mat, annot = True, cmap = cmap, fmt = '.2f', center = 0,\n            annot_kws = {'size': 12}, mask = data_mask).set_title('Correlations Among Input Features');\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:20:00.537152Z","iopub.execute_input":"2023-10-03T01:20:00.537498Z","iopub.status.idle":"2023-10-03T01:20:02.668766Z","shell.execute_reply.started":"2023-10-03T01:20:00.537474Z","shell.execute_reply":"2023-10-03T01:20:02.667624Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above, these are couple of observations:\n\n- There is a 97% correlation between `branchCount` and `v(g)`.\n- There is a 96% correlation between `total_Opnd` and `total_Op`\n- There is a 96% correlation between `total_Op` and `n`\n- `l` is the only feature that is negative correlated with the other features.\n\nBased on the above correlation heatmap, we proceed to explore the idea of dimension reduction via `PCA`.","metadata":{}},{"cell_type":"code","source":"pca_md = Pipeline([('scaler', StandardScaler()), \n                   ('PCA', PCA())]).fit(train.drop(columns = ['id', 'defects'], axis = 1))\npca_md","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:48:52.478145Z","iopub.execute_input":"2023-10-03T01:48:52.478514Z","iopub.status.idle":"2023-10-03T01:48:52.650583Z","shell.execute_reply.started":"2023-10-03T01:48:52.478488Z","shell.execute_reply":"2023-10-03T01:48:52.649215Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (10, 8))\nax = sns.lineplot(x = [i for i in range(1, 22)], y = np.cumsum(pca_md.named_steps['PCA'].explained_variance_ratio_), color = 'steelblue', markers = True);\nax.set_xlabel('Number of Components')\nax.set_ylabel('Explained Variance (%)')\nax.set_xticks([i for i in range(1, 22)]);","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:56:28.817215Z","iopub.execute_input":"2023-10-03T01:56:28.817654Z","iopub.status.idle":"2023-10-03T01:56:29.173087Z","shell.execute_reply.started":"2023-10-03T01:56:28.817622Z","shell.execute_reply":"2023-10-03T01:56:29.171918Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above, 15 components explain about 98.5% of the variability in the data. Next, we explore a couple of bivariate relationships as shown below.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize = (20,8))\n\nsns.scatterplot(ax = axes[0], data = train, x = 'uniq_Op', y = 'uniq_Opnd', hue = 'defects');\nsns.scatterplot(ax = axes[1], data = train, x = 'total_Op', y = 'total_Opnd', hue = 'defects');","metadata":{"execution":{"iopub.status.busy":"2023-10-03T02:13:42.763715Z","iopub.execute_input":"2023-10-03T02:13:42.7642Z","iopub.status.idle":"2023-10-03T02:13:53.207137Z","shell.execute_reply.started":"2023-10-03T02:13:42.764158Z","shell.execute_reply":"2023-10-03T02:13:53.20614Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above plots, these are a couple of observations:\n\n- There are several outliers in this data. \n- There is not a clear pattern that can be leveraged to separate the two classes.\n\nNext, we count the number of unique values in each of the input features as follows.","metadata":{}},{"cell_type":"code","source":"train.drop(columns = ['id', 'defects'], axis = 1).nunique()","metadata":{"execution":{"iopub.status.busy":"2023-10-03T02:22:22.265969Z","iopub.execute_input":"2023-10-03T02:22:22.266424Z","iopub.status.idle":"2023-10-03T02:22:22.310108Z","shell.execute_reply.started":"2023-10-03T02:22:22.266392Z","shell.execute_reply":"2023-10-03T02:22:22.308725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above, notice that `locCodeAndComment` is the feature with the least number of unique values; it has only 29 unique values. ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n# <h1 style=\"background-color:lightgray;font-family:newtimeroman;font-size:350%;text-align:center;border-radius: 15px 50px;\">Baseline Modeling 1.0</h1>\n\nFirst, we start by building some standard models without feature engineering nor HPO. First, we define the input and target features.","metadata":{}},{"cell_type":"code","source":"X = train.drop(columns = ['id', 'defects'], axis = 1)\nY = train['defects'].map({False: 0, True: 1})\n\ntest_cv = test.drop(columns = ['id'], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:13:57.433113Z","iopub.execute_input":"2023-10-03T14:13:57.433542Z","iopub.status.idle":"2023-10-03T14:13:57.472457Z","shell.execute_reply.started":"2023-10-03T14:13:57.433512Z","shell.execute_reply":"2023-10-03T14:13:57.470463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First, we define the hill ensemble function to ensemble model predictions.","metadata":{}},{"cell_type":"code","source":"def hill_climbing(x, y, x_test):\n    \n    # Evaluating oof predictions\n    scores = {}\n    for col in x.columns:\n        scores[col] = roc_auc_score(y, x[col])\n\n    # Sorting the model scores\n    scores = {k: v for k, v in sorted(scores.items(), key = lambda item: item[1], reverse = True)}\n\n    # Sort oof_df and test_preds\n    x = x[list(scores.keys())]\n    x_test = x_test[list(scores.keys())]\n\n    STOP = False\n    current_best_ensemble = x.iloc[:,0]\n    current_best_test_preds = x_test.iloc[:,0]\n    MODELS = x.iloc[:,1:]\n    weight_range = np.arange(-0.5, 0.51, 0.01) \n    history = [roc_auc_score(y, current_best_ensemble)]\n    j = 0\n\n    while not STOP:\n        j += 1\n        potential_new_best_cv_score = roc_auc_score(y, current_best_ensemble)\n        k_best, wgt_best = None, None\n        for k in MODELS:\n            for wgt in weight_range:\n                potential_ensemble = (1 - wgt) * current_best_ensemble + wgt * MODELS[k]\n                cv_score = roc_auc_score(y, potential_ensemble)\n                if cv_score > potential_new_best_cv_score:\n                    potential_new_best_cv_score = cv_score\n                    k_best, wgt_best = k, wgt\n\n        if k_best is not None:\n            current_best_ensemble = (1 - wgt_best) * current_best_ensemble + wgt_best * MODELS[k_best]\n            current_best_test_preds = (1 - wgt_best) * current_best_test_preds + wgt_best * x_test[k_best]\n            MODELS.drop(k_best, axis = 1, inplace = True)\n            if MODELS.shape[1] == 0:\n                STOP = True\n            history.append(potential_new_best_cv_score)\n        else:\n            STOP = True\n        \n    hill_ens_pred_1 = current_best_ensemble\n    hill_ens_pred_2 = current_best_test_preds\n    \n    return [hill_ens_pred_1, hill_ens_pred_2]","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:15:49.272764Z","iopub.execute_input":"2023-10-03T14:15:49.273178Z","iopub.status.idle":"2023-10-03T14:15:49.285102Z","shell.execute_reply.started":"2023-10-03T14:15:49.273149Z","shell.execute_reply":"2023-10-03T14:15:49.283482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we build a few standard models in a 10-fold cross-validation routine.","metadata":{}},{"cell_type":"code","source":"ens_cv_scores, ens_preds = list(), list()\nhill_ens_cv_scores, hill_ens_preds =  list(), list()\n\nsk = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 1, random_state = 42)\nfor i, (train_idx, test_idx) in enumerate(sk.split(X, Y)):\n\n    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n    Y_train, Y_test = Y.iloc[train_idx], Y.iloc[test_idx]\n    \n    print('----------------------------------------------------------')\n    \n    ########\n    ## RF ##\n    ########\n\n    RF_md = RandomForestClassifier(n_estimators = 500, \n                                   max_depth = 7,\n                                   min_samples_split = 15,\n                                   min_samples_leaf = 10).fit(X_train, Y_train)\n    \n    RF_pred = RF_md.predict_proba(X_test)[:, 1]\n    RF_score = roc_auc_score(Y_test, RF_pred)\n\n    print('Fold', i, '==> RF oof ROC-AUC score is ==>', RF_score)\n\n    RF_pred_test = RF_md.predict_proba(test_cv)[:, 1]\n    \n    #################\n    ## Extra Trees ##\n    #################\n\n    ET_md = ExtraTreesClassifier(n_estimators = 500, \n                                 max_depth = 7,\n                                 min_samples_split = 15,\n                                 min_samples_leaf = 10).fit(X_train, Y_train)\n\n    ET_pred = ET_md.predict_proba(X_test)[:, 1]\n    ET_score = roc_auc_score(Y_test, ET_pred)\n\n    print('Fold', i, '==> ET oof ROC-AUC score is ==>', ET_score)\n\n    ET_pred_test = ET_md.predict_proba(test_cv)[:, 1]\n\n    ##########################\n    ## HistGradientBoosting ##\n    ##########################\n\n    hist_md = HistGradientBoostingClassifier(l2_regularization = 0.01,\n                                             early_stopping = False,\n                                             learning_rate = 0.01,\n                                             max_iter = 500,\n                                             max_depth = 5,\n                                             max_bins = 255,\n                                             min_samples_leaf = 15,\n                                             max_leaf_nodes = 10).fit(X_train, Y_train)\n    \n    hist_pred = hist_md.predict_proba(X_test)[:, 1]\n    hist_score = roc_auc_score(Y_test, hist_pred)\n\n    print('Fold', i, '==> Hist oof ROC-AUC score is ==>', hist_score)  \n\n    hist_pred_test = hist_md.predict_proba(test_cv)[:, 1]\n\n    ##########\n    ## LGBM ##\n    ##########\n\n    LGBM_md = LGBMClassifier(objective = 'binary',\n                             n_estimators = 500,\n                             max_depth = 7,\n                             learning_rate = 0.01,\n                             num_leaves = 20,\n                             reg_alpha = 3,\n                             reg_lambda = 3,\n                             subsample = 0.7,\n                             colsample_bytree = 0.7).fit(X_train, Y_train)\n\n    lgb_pred = LGBM_md.predict_proba(X_test)[:, 1]\n    lgb_score = roc_auc_score(Y_test, lgb_pred)\n\n    print('Fold', i, '==> LGBM oof ROC-AUC score is ==>', lgb_score) \n\n    lgb_pred_test = LGBM_md.predict_proba(test_cv)[:, 1]\n\n    #########\n    ## XGB ##\n    #########\n\n    XGB_md = XGBClassifier(objective = 'binary:logistic',\n                           tree_method = 'hist',\n                           colsample_bytree = 0.7, \n                           gamma = 2, \n                           learning_rate = 0.01, \n                           max_depth = 7, \n                           min_child_weight = 10, \n                           n_estimators = 500, \n                           subsample = 0.7).fit(X_train, Y_train)\n\n    xgb_pred = XGB_md.predict_proba(X_test)[:, 1]\n    xgb_score = roc_auc_score(Y_test, xgb_pred)\n\n    print('Fold', i, '==> XGB oof ROC-AUC score is ==>', xgb_score)\n\n    xgb_pred_test = XGB_md.predict_proba(test_cv)[:, 1]\n\n    ##############\n    ## CatBoost ##\n    ##############\n\n    Cat_md = CatBoostClassifier(loss_function = 'Logloss',\n                                iterations = 500,\n                                learning_rate = 0.01,\n                                depth = 7,\n                                random_strength = 0.5,\n                                bagging_temperature = 0.7,\n                                border_count = 30,\n                                l2_leaf_reg = 5,\n                                verbose = False, \n                                task_type = 'CPU').fit(X_train, Y_train)\n\n    cat_pred = Cat_md.predict_proba(X_test)[:, 1]\n    cat_score = roc_auc_score(Y_test, cat_pred)\n\n    print('Fold', i, '==> CatBoost oof ROC-AUC score is ==>', cat_score)\n\n    cat_pred_test = Cat_md.predict_proba(test_cv)[:, 1]    \n    \n    ##############\n    ## Ensemble ##\n    ##############\n    \n    ens_pred_1 = (RF_pred + ET_pred + hist_pred + lgb_pred + xgb_pred + cat_pred) / 6\n    ens_pred_2 = (RF_pred_test + ET_pred_test + hist_pred_test + lgb_pred_test + xgb_pred_test + cat_pred_test) / 6\n    \n    ens_score_fold = roc_auc_score(Y_test, ens_pred_1)\n    ens_cv_scores.append(ens_score_fold)\n    ens_preds.append(ens_pred_2)\n    \n    print('Fold', i, '==> Average Ensemble oof ROC-AUC score is ==>', ens_score_fold)\n    \n    ############################\n    ## Hill Climbing Ensemble ##\n    ############################\n    \n    x = pd.DataFrame({'RF': RF_pred,\n                      'ET': ET_pred, \n                      'Hist': hist_pred, \n                      'LGBM': lgb_pred,\n                      'XGB': xgb_pred,\n                      'Cat': cat_pred})\n    y = Y_test\n        \n    x_test = pd.DataFrame({'RF': RF_pred_test,\n                           'ET': ET_pred_test, \n                           'Hist': hist_pred_test, \n                           'LGBM': lgb_pred_test,\n                           'XGB': xgb_pred_test,\n                           'Cat': cat_pred_test})\n    \n    hill_results = hill_climbing(x, y, x_test)\n    \n    hill_ens_score_fold = roc_auc_score(y, hill_results[0])\n    hill_ens_cv_scores.append(hill_ens_score_fold)\n    hill_ens_preds.append(hill_results[1])\n\n    print('Fold', i, '==> Hill Climbing Ensemble oof ROC-AUC score is ==>', hill_ens_score_fold)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:16:28.198786Z","iopub.execute_input":"2023-10-03T14:16:28.199254Z","iopub.status.idle":"2023-10-03T14:34:43.930803Z","shell.execute_reply.started":"2023-10-03T14:16:28.199223Z","shell.execute_reply":"2023-10-03T14:34:43.929705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('The average ensemble oof ROC-AUC score over the 10-folds is', np.mean(ens_cv_scores))\nprint('The hill climbing ensemble oof ROC-AUC score over the 10-folds is', np.mean(hill_ens_cv_scores))","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:35:13.756587Z","iopub.execute_input":"2023-10-03T14:35:13.756941Z","iopub.status.idle":"2023-10-03T14:35:13.764586Z","shell.execute_reply.started":"2023-10-03T14:35:13.756914Z","shell.execute_reply":"2023-10-03T14:35:13.762921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ens_preds_test = pd.DataFrame(ens_preds).apply(np.mean, axis = 0)\n\nsubmission['defects'] = ens_preds_test\nsubmission.to_csv('Avereage_Ensemble_Baseline_submission.csv', index = False)\n\nens_preds_test = pd.DataFrame(hill_ens_preds).apply(np.mean, axis = 0)\n\nsubmission['defects'] = ens_preds_test\nsubmission.to_csv('Hill_Ensemble_Baseline_submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T03:11:01.836641Z","iopub.execute_input":"2023-10-03T03:11:01.837124Z","iopub.status.idle":"2023-10-03T03:11:08.502184Z","shell.execute_reply.started":"2023-10-03T03:11:01.837092Z","shell.execute_reply":"2023-10-03T03:11:08.501115Z"},"trusted":true},"execution_count":null,"outputs":[]}]}